{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How To Use Google's Word2Vec C Source File\n",
    "\n",
    "How to use Google's Word2Vec c source file.\n",
    "\n",
    "you can download Word2Vec c source file from https://code.google.com/archive/p/word2vec/source/default/source.\n",
    "\n",
    "Also Google's code archive of Word2Vec is : https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "## How to use it \n",
    "\n",
    "\n",
    "### download\n",
    "\n",
    "you cannot export this source files to your github, so you just have to download this source files directly.\n",
    "\n",
    "### unzip\n",
    "\n",
    "After Downloading the above code, uncompress the download file. \n",
    "\n",
    "```bash\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test [17:26:31] \n",
    "$ ls\n",
    "source-archive.zip\n",
    "```\n",
    "\n",
    "> $ unzip source-archive.zip\n",
    "\n",
    "```bash\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test [17:27:41] \n",
    "$ unzip source-archive.zip \n",
    "Archive:  source-archive.zip\n",
    "   creating: word2vec/\n",
    "   creating: word2vec/.svn/\n",
    "  inflating: word2vec/.svn/all-wcprops  \n",
    "   creating: word2vec/.svn/tmp/\n",
    "   creating: word2vec/.svn/tmp/prop-base/\n",
    "   creating: word2vec/.svn/tmp/props/\n",
    "   creating: word2vec/.svn/tmp/text-base/\n",
    "   creating: word2vec/.svn/prop-base/\n",
    "   creating: word2vec/.svn/props/\n",
    "   creating: word2vec/.svn/text-base/\n",
    "  inflating: word2vec/.svn/entries   \n",
    "   creating: word2vec/trunk/\n",
    "  inflating: word2vec/trunk/demo-phrases.sh  \n",
    "  inflating: word2vec/trunk/demo-word.sh  \n",
    "  inflating: word2vec/trunk/questions-phrases.txt  \n",
    "  inflating: word2vec/trunk/demo-word-accuracy.sh  \n",
    "  inflating: word2vec/trunk/README.txt  \n",
    "  inflating: word2vec/trunk/makefile  \n",
    "  inflating: word2vec/trunk/word2phrase.c  \n",
    "  inflating: word2vec/trunk/compute-accuracy.c  \n",
    "   creating: word2vec/trunk/.svn/\n",
    "  inflating: word2vec/trunk/.svn/all-wcprops  \n",
    "   creating: word2vec/trunk/.svn/tmp/\n",
    "   creating: word2vec/trunk/.svn/tmp/prop-base/\n",
    "   creating: word2vec/trunk/.svn/tmp/props/\n",
    "   creating: word2vec/trunk/.svn/tmp/text-base/\n",
    "   creating: word2vec/trunk/.svn/prop-base/\n",
    " extracting: word2vec/trunk/.svn/prop-base/demo-train-big-model-v1.sh.svn-base  \n",
    "   creating: word2vec/trunk/.svn/props/\n",
    "   creating: word2vec/trunk/.svn/text-base/\n",
    "  inflating: word2vec/trunk/.svn/text-base/word2phrase.c.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/questions-words.txt.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/demo-classes.sh.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/makefile.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/compute-accuracy.c.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/distance.c.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/demo-analogy.sh.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/README.txt.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/word2vec.c.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/demo-phrases.sh.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/demo-train-big-model-v1.sh.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/demo-phrase-accuracy.sh.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/questions-phrases.txt.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/LICENSE.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/word-analogy.c.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/demo-word-accuracy.sh.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/text-base/demo-word.sh.svn-base  \n",
    "  inflating: word2vec/trunk/.svn/entries  \n",
    "  inflating: word2vec/trunk/demo-classes.sh  \n",
    "  inflating: word2vec/trunk/demo-analogy.sh  \n",
    "  inflating: word2vec/trunk/demo-train-big-model-v1.sh  \n",
    "  inflating: word2vec/trunk/word-analogy.c  \n",
    "  inflating: word2vec/trunk/demo-phrase-accuracy.sh  \n",
    "  inflating: word2vec/trunk/distance.c  \n",
    "  inflating: word2vec/trunk/word2vec.c  \n",
    "  inflating: word2vec/trunk/questions-words.txt  \n",
    "  inflating: word2vec/trunk/LICENSE  \n",
    "```\n",
    "\n",
    "After unziping **source-archive.zip**, you can see some directory\n",
    "\n",
    "I mean word2vec directory is created. so if you enter the directory. \n",
    "\n",
    "you see a directory, trunk. just continuously enter in. \n",
    "\n",
    "Finally You find out C source file of word2vec of google\n",
    "\n",
    "let's see the processing of what I said above in the following :\n",
    "\n",
    "```bash\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test [17:28:03] \n",
    "$ ls\n",
    "source-archive.zip  word2vec\n",
    "\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test [17:28:49] \n",
    "$ cd word2vec \n",
    "\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test/word2vec [17:29:04] \n",
    "$ ls\n",
    "trunk\n",
    "\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test/word2vec [17:29:05] \n",
    "$ cd trunk \n",
    "\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test/word2vec/trunk [17:29:10] \n",
    "$ ls\n",
    "compute-accuracy.c       demo-train-big-model-v1.sh  makefile               word2vec.c\n",
    "demo-analogy.sh          demo-word-accuracy.sh       questions-phrases.txt  word-analogy.c\n",
    "demo-classes.sh          demo-word.sh                questions-words.txt\n",
    "demo-phrase-accuracy.sh  distance.c                  README.txt\n",
    "demo-phrases.sh          LICENSE                     word2phrase.c\n",
    "```\n",
    "\n",
    "### Make and a example with demo-analogy.sh \n",
    "\n",
    "In order to compile the Word2Vec source, use **make** \n",
    "\n",
    "```bash\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test/word2vec/trunk [17:33:25] \n",
    "$ make\n",
    "gcc word2vec.c -o word2vec -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result\n",
    "gcc word2phrase.c -o word2phrase -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result\n",
    "gcc distance.c -o distance -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result\n",
    "distance.c: In function ‘main’:\n",
    "distance.c:31:8: warning: unused variable ‘ch’ [-Wunused-variable]\n",
    "   char ch;\n",
    "        ^\n",
    "gcc word-analogy.c -o word-analogy -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result\n",
    "word-analogy.c: In function ‘main’:\n",
    "word-analogy.c:31:8: warning: unused variable ‘ch’ [-Wunused-variable]\n",
    "   char ch;\n",
    "        ^\n",
    "gcc compute-accuracy.c -o compute-accuracy -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result\n",
    "compute-accuracy.c: In function ‘main’:\n",
    "compute-accuracy.c:29:109: warning: unused variable ‘ch’ [-Wunused-variable]\n",
    " [max_size], st2[max_size], st3[max_size], st4[max_size], bestw[N][max_size], file_name[max_size], ch;\n",
    "                                                                                                   ^\n",
    "chmod +x *.sh\n",
    "\n",
    "\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/test/word2vec/trunk [17:38:36] \n",
    "$ ls\n",
    "compute-accuracy         demo-train-big-model-v1.sh  makefile               word2vec\n",
    "compute-accuracy.c       demo-word-accuracy.sh       questions-phrases.txt  word2vec.c\n",
    "demo-analogy.sh          demo-word.sh                questions-words.txt    word-analogy\n",
    "demo-classes.sh          distance                    README.txt             word-analogy.c\n",
    "demo-phrase-accuracy.sh  distance.c                  word2phrase\n",
    "demo-phrases.sh          LICENSE                     word2phrase.c\n",
    "```\n",
    "\n",
    "As you can see abvoe, after make command, according to chmod +x *.sh, You can execute all shell scripts. \n",
    "\n",
    "From now on, let's walk through a shell scripts. \n",
    "\n",
    "One of demo shell scripts we are talking about is demo-analogy to find out analogy of the relationship of words like Man - Woman + King = Queen :\n",
    "\n",
    "> $ vim demo-analogy.sh\n",
    "\n",
    "```shell\n",
    "make\n",
    "if [ ! -e text8 ]; then\n",
    "  wget http://mattmahoney.net/dc/text8.zip -O text8.gz\n",
    "  gzip -d text8.gz -f\n",
    "fi\n",
    "echo ---------------------------------------------------------------------------------------------------\n",
    "echo Note that for the word analogy to perform well, the model should be trained on much larger data set\n",
    "echo Example input: paris france berlin\n",
    "echo ---------------------------------------------------------------------------------------------------\n",
    "time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15\n",
    "./word-analogy vectors.bin            \n",
    "```\n",
    "\n",
    "As you can see the above snippets of all shell scripts. you get a hint of how to make word2vec, it show you a line, \"time ./word2vec....\" :\n",
    "\n",
    "> time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15\n",
    "\n",
    "**Keep in mind, size is the size of your word-embedding. I mean word dimension.**\n",
    "\n",
    "Let's execute the demo-word.sh\n",
    "\n",
    "> $ ./demo-analogy.sh\n",
    "\n",
    "```bash\n",
    "$ ./demo-analogy.sh \n",
    "make: Nothing to be done for 'all'.\n",
    "--2017-11-15 18:58:49--  http://mattmahoney.net/dc/text8.zip\n",
    "Resolving mattmahoney.net (mattmahoney.net)... 98.139.135.129\n",
    "Connecting to mattmahoney.net (mattmahoney.net)|98.139.135.129|:80... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 31344016 (30M) [application/zip]\n",
    "Saving to: ‘text8.gz’\n",
    "\n",
    "text8.gz                   100%[=====================================>]  29.89M   220KB/s    in 2m 17s  \n",
    "\n",
    "2017-11-15 19:01:06 (224 KB/s) - ‘text8.gz’ saved [31344016/31344016]\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n",
    "Note that for the word analogy to perform well, the model should be trained on much larger data set\n",
    "Example input: paris france berlin\n",
    "---------------------------------------------------------------------------------------------------\n",
    "Starting training using file text8\n",
    "Vocab size: 71291\n",
    "Words in train file: 16718843\n",
    "Alpha: 0.000005  Progress: 100.10%  Words/thread/sec: 79.71k  3147.71user 4.74system 7:43.00elapsed 680%CPU (0avgtext+0avgdata 652360maxresident)k\n",
    "0inputs+112712outputs (0major+11111minor)pagefaults 0swaps\n",
    "Enter three words (EXIT to break): man woman king\n",
    "\n",
    "Word: man  Position in vocabulary: 243\n",
    "\n",
    "Word: woman  Position in vocabulary: 1013\n",
    "\n",
    "Word: king  Position in vocabulary: 187\n",
    "\n",
    "                                              Word              Distance\n",
    "------------------------------------------------------------------------\n",
    "                                             queen\t\t0.631099\n",
    "                                           matilda\t\t0.520393\n",
    "                                           marries\t\t0.511200\n",
    "                                           heiress\t\t0.504250\n",
    "                                             anjou\t\t0.502568\n",
    "                                          daughter\t\t0.496327\n",
    "                                          philippa\t\t0.491124\n",
    "                                            valois\t\t0.487329\n",
    "                                               vii\t\t0.485470\n",
    "                                         melisende\t\t0.483603\n",
    "                                         betrothed\t\t0.483228\n",
    "                                         aquitaine\t\t0.471253\n",
    "                                           consort\t\t0.469374\n",
    "                                         elizabeth\t\t0.466588\n",
    "                                                xv\t\t0.464804\n",
    "                                        montferrat\t\t0.462666\n",
    "                                           yolande\t\t0.462272\n",
    "                                           jadwiga\t\t0.457727\n",
    "                                            throne\t\t0.457579\n",
    "                                               xiv\t\t0.453404\n",
    "                                            boleyn\t\t0.450848\n",
    "                                           castile\t\t0.449824\n",
    "                                              wife\t\t0.448622\n",
    "                                       plantagenet\t\t0.448620\n",
    "                                            aragon\t\t0.448198\n",
    "                                           infanta\t\t0.448098\n",
    "                                          sicilies\t\t0.445172\n",
    "                                          isabella\t\t0.442505\n",
    "                                           duchess\t\t0.440818\n",
    "                                          burgundy\t\t0.439758\n",
    "                                           monarch\t\t0.439106\n",
    "                                           sibylla\t\t0.438516\n",
    "                                               xvi\t\t0.435970\n",
    "                                          eleonora\t\t0.435931\n",
    "                                              vasa\t\t0.435576\n",
    "                                          athaliah\t\t0.434251\n",
    "                                           jezebel\t\t0.434112\n",
    "                                          eleonore\t\t0.433874\n",
    "                                             savoy\t\t0.432848\n",
    "                                             kings\t\t0.432686\n",
    "Enter three words (EXIT to break): \n",
    "```\n",
    "\n",
    "> $ ls\n",
    "\n",
    "```bash\n",
    "$ ls\n",
    "compute-accuracy         demo-train-big-model-v1.sh  makefile               word2phrase\n",
    "compute-accuracy.c       demo-word-accuracy.sh       questions-phrases.txt  word2phrase.c\n",
    "demo-analogy.sh          demo-word.sh                questions-words.txt    word2vec\n",
    "demo-classes.sh          distance                    README.txt             word2vec.c\n",
    "demo-phrase-accuracy.sh  distance.c                  text8                  word-analogy\n",
    "demo-phrases.sh          LICENSE                     vectors.bin            word-analogy.c\n",
    "```\n",
    "\n",
    "as you can see above, you could find out something like **vectors.bin** \n",
    "\n",
    "thai is file that stores vector values\n",
    "\n",
    "### help message  \n",
    "\n",
    "But you don't need to analyze shell script. just word2vec executable show you how to use word2vec executable as you type **./word2vec** in command line like this : \n",
    "\n",
    "> $ ./word2vec\n",
    "\n",
    "```bash \n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/my-jupyter/word2vec-of-google/word2vec/trunk [19:10:08] C:127\n",
    "$ ./word2vec \n",
    "WORD VECTOR estimation toolkit v 0.1c\n",
    "\n",
    "Options:\n",
    "Parameters for training:\n",
    "\t-train <file>\n",
    "\t\tUse text data from <file> to train the model\n",
    "\t-output <file>\n",
    "\t\tUse <file> to save the resulting word vectors / word clusters\n",
    "\t-size <int>\n",
    "\t\tSet size of word vectors; default is 100\n",
    "\t-window <int>\n",
    "\t\tSet max skip length between words; default is 5\n",
    "\t-sample <float>\n",
    "\t\tSet threshold for occurrence of words. Those that appear with higher frequency in the training data\n",
    "\t\twill be randomly down-sampled; default is 1e-3, useful range is (0, 1e-5)\n",
    "\t-hs <int>\n",
    "\t\tUse Hierarchical Softmax; default is 0 (not used)\n",
    "\t-negative <int>\n",
    "\t\tNumber of negative examples; default is 5, common values are 3 - 10 (0 = not used)\n",
    "\t-threads <int>\n",
    "\t\tUse <int> threads (default 12)\n",
    "\t-iter <int>\n",
    "\t\tRun more training iterations (default 5)\n",
    "\t-min-count <int>\n",
    "\t\tThis will discard words that appear less than <int> times; default is 5\n",
    "\t-alpha <float>\n",
    "\t\tSet the starting learning rate; default is 0.025 for skip-gram and 0.05 for CBOW\n",
    "\t-classes <int>\n",
    "\t\tOutput word classes rather than word vectors; default number of classes is 0 (vectors are written)\n",
    "\t-debug <int>\n",
    "\t\tSet the debug mode (default = 2 = more info during training)\n",
    "\t-binary <int>\n",
    "\t\tSave the resulting vectors in binary moded; default is 0 (off)\n",
    "\t-save-vocab <file>\n",
    "\t\tThe vocabulary will be saved to <file>\n",
    "\t-read-vocab <file>\n",
    "\t\tThe vocabulary will be read from <file>, not constructed from the training data\n",
    "\t-cbow <int>\n",
    "\t\tUse the continuous bag of words model; default is 1 (use 0 for skip-gram model)\n",
    "\n",
    "Examples:\n",
    "./word2vec -train data.txt -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3\n",
    "```\n",
    "\n",
    "In here, simply speaking about word2vec usage. \n",
    "\n",
    "- data.txt means a file you want to train for word embedding\n",
    "\n",
    "- binary : whether output is binary or not\n",
    "\n",
    "Let's see vector.txt using word2vec of google. \n",
    "\n",
    "> $ time ./word2vec -train text8 -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3\n",
    "\n",
    "```bash\n",
    "# hyunyoung2 @ hyunyoung2-desktop in ~/before-ubutu/my-jupyter/word2vec-of-google/test/word2vec/trunk [19:16:18] C:1\n",
    "$ ./word2vec -train text8 -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3\n",
    "Starting training using file text8\n",
    "Vocab size: 71291\n",
    "Words in train file: 16718843\n",
    "Alpha: 0.000005  Progress: 100.03%  Words/thread/sec: 298.74k  %  \n",
    "```\n",
    "\n",
    "> $ vim vec.txt\n",
    "\n",
    "```\n",
    "71291 200\n",
    "</s> 0.002001 0.002210 -0.001915 -0.001639 0.000683 0.001511 0.000470 0.000106 -0.001802 0.001109 -0.002178 0.000625 -0.000376 -0.000479 -0.001658 -0.000941 0.001290 0.001513 0.001485 0.000799 0.000772 -0.001901 -0.002048 0.002485 0.001901 0.001545 -0.000302 0.002008 -0.000247 0.000367 -0.000075 -0.001492 0.000656 -0.000669 -0.001913 0.002377 0.002190 -0.000548 -0.000113 0.000255 -0.001819 -0.002004 0.002277 0.000032 -0.001291 -0.001521 -0.001538 0.000848 0.000101 0.000666 -0.002107 -0.001904 -0.000065 0.000572 0.001275 -0.001585 0.002040 0.000463 0.000560 -0.000304 0.001493 -0.001144 -0.001049 0.001079 -0.000377 0.000515 0.000902 -0.002044 -0.000992 0.001457 0.002116 0.001966 -0.001523 -0.001054 -0.000455 0.001001 -0.001894 0.001499 0.001394 -0.000799 -0.000776 -0.001119 0.002114 0.001956 -0.000590 0.002107 0.002410 0.000908 0.002491 -0.001556 -0.000766 -0.001054 -0.001454 0.001407 0.000790 0.000212 -0.001097 0.000762 0.001530 0.0\n",
    "```\n",
    "\n",
    "As you can see above, you can check the total number of word vector' and dimensions of a word. and then  you can identify the vector value of a word like this :\n",
    "\n",
    "> 71291(total count of word vector) 200(dimension of a word)\n",
    "> ### The following is vector value of </s>\n",
    "> </s> 0.002001 0.002210 -0.001915 -0.001639 0.000683 0.001511 0.000470 ......\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
