{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Data Reading \n",
    "# Import  Data for Spam or Ham \n",
    "import numpy as np\n",
    "def csv_to_numpy_array(filePath, delimiter):\n",
    "    return np.genfromtxt(filePath, delimiter=delimiter, dtype=None)\n",
    "\n",
    "def import_data() :\n",
    "    print (\"Loading data...\")\n",
    "    trainX_path = \"/home/hyunyoung2/my-jupyter/spam-filter/Project/Project_code_spam/tensorflow-tutorial/data/trainX.csv\"\n",
    "    trainY_path = \"/home/hyunyoung2/my-jupyter/spam-filter/Project/Project_code_spam/tensorflow-tutorial/data/trainY.csv\"\n",
    "    testX_path = \"/home/hyunyoung2/my-jupyter/spam-filter/Project/Project_code_spam/tensorflow-tutorial/data/testX.csv\"\n",
    "    testY_path = \"/home/hyunyoung2/my-jupyter/spam-filter/Project/Project_code_spam/tensorflow-tutorial/data/testY.csv\"\n",
    "    trainX = csv_to_numpy_array(trainX_path, delimiter=\"\\t\")\n",
    "    trainY = csv_to_numpy_array(trainY_path, delimiter=\"\\t\")\n",
    "    testX = csv_to_numpy_array(testX_path, delimiter=\"\\t\")\n",
    "    testY = csv_to_numpy_array(testY_path, delimiter=\"\\t\")\n",
    "    \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test 2-2 (Activation)\n",
    "## The number of epoch : 27000\n",
    "## cost function : cross entropy\n",
    "## activation function : tanh\n",
    "## Update Method : GradientDscent\n",
    "## The number of Hidden_Layer : 1\n",
    "\n",
    "trainX, trainY, testX, testY = import_data()\n",
    "\n",
    "print (\"Import was done : \")\n",
    "print (\"trainX shape :\",trainX.shape)\n",
    "print (\"trainY shape :\",trainY.shape)\n",
    "print (\"testX shape :\",testX.shape)\n",
    "print (\"testY shape :\",testY.shape)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameter \n",
    "traing_epochs = 27000\n",
    "## Step size = A smarter Learing rate for gradientOptimizer\n",
    "## Basically, You can use a constant like \"learning_rate = 0.0008\"\n",
    "training_learing_rate = 0.0008 #tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                               #          global_step=1,\n",
    "                               #          decay_steps=trainX.shape[0],\n",
    "                               #          decay_rate = 0.95,\n",
    "                               #          staircase=True)\n",
    "# Network Parameter\n",
    "## z = a(W_1*x+b_1)\n",
    "## Y = W_2*z + b_2\n",
    "n_hidden_1 = trainX.shape[1] # Currently, the size of Word2Vec is 300\n",
    "                               # when you know about word2Vec size like the above \"n_hidden\"\n",
    "logs_path = '/home/hyunyoung2/my-jupyter/tensorflow/test_example/HCTL_2017_About_DeepLearning_2/3rd_copy/'\n",
    "\n",
    "n_inputs = trainX.shape[1] # In other words, Input size is 300 \n",
    "print (\"n_inputs :\", n_inputs)\n",
    "## Currently, n_label is a couple like two-dimension [0, 1]\n",
    "## But later on, you can change the type of classes like below\n",
    "## \"n_classes\" \n",
    "n_labels = trainY.shape[1]\n",
    "print (\"n_labels : \",  n_labels)\n",
    "\n",
    "# tf Graph input : inputs and ground_truth\n",
    "x = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "ground_truth = tf.placeholder(tf.float32, [None, n_labels])\n",
    "\n",
    "print (\"starting store layers... \")\n",
    "# Store layers weights & bias\n",
    "weights = {\n",
    "    \"h1\" : tf.Variable(tf.random_normal([n_inputs, n_hidden_1], mean=0,\n",
    "                                        stddev=(np.sqrt(6/(n_inputs + n_hidden_1+1))),\n",
    "                                        name = \"weight_h1\")),\n",
    "   \n",
    "    \"out\" : tf.Variable(tf.random_normal([n_hidden_1, n_labels], mean=0,\n",
    "                                        stddev=(np.sqrt(6/(n_hidden_1 + n_labels+1))),\n",
    "                                        name = \"weight_out\"))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"b1\" : tf.Variable(tf.random_normal([1, n_hidden_1],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/(n_inputs + n_hidden_1+1))),\n",
    "                                        name = \"bias_h1\")),\n",
    "    \n",
    "    \"out\" : tf.Variable(tf.random_normal([1, n_labels],\n",
    "                                         mean=0,\n",
    "                                         stddev=(np.sqrt(6/(n_hidden_1 + n_labels+1))),\n",
    "                                         name = \"bias_out\"))                                   \n",
    "}\n",
    "\n",
    "print(\"Starting Fully_connected_NN...\")\n",
    "# Create Model\n",
    "def multilayer_fully_connected_NN(x, weight, biases) :\n",
    "    # Hidden layer with Sigmoid Activation\n",
    "    layer_1_biases = tf.add(tf.matmul(x, weight[\"h1\"]), biases[\"b1\"])\n",
    "    print (\"layer_1_biases : \", layer_1_biases.shape)\n",
    "    layer_1 = tf.nn.tanh(layer_1_biases)\n",
    "    \n",
    "    out_layer_final = tf.add(tf.matmul(layer_1, weight[\"out\"]), biases[\"out\"])\n",
    "    print (\"weight['out'] :\", weight[\"out\"].shape)\n",
    "    print (\"out_layer_final : \", out_layer_final.shape)\n",
    "    #out_layer = tf.nn.sigmoid(out_layer_biases)\n",
    "    #print (\"out_layer :\", out_layer.shape)\n",
    "    \n",
    "    return out_layer_final\n",
    "\n",
    "# Predict with a model you contruct \n",
    "training_prediction = multilayer_fully_connected_NN(x, weights, biases)\n",
    "# Construct model\n",
    "\n",
    "pred = tf.nn.softmax(training_prediction) # Softmax\n",
    "\n",
    "# Define loss and optimizer \n",
    "#training_cost = tf.nn.l2_loss((training_prediction-ground_truth), name=\"squared_error_cost\")\n",
    "training_cost = tf.reduce_mean(-tf.reduce_sum(ground_truth*tf.log(pred), reduction_indices=[1]))\n",
    "training_optimzer = tf.train.GradientDescentOptimizer(learning_rate=training_learing_rate).minimize(training_cost)\n",
    "#training_optimzer = tf.train.AdamOptimizer(learning_rate=training_learing_rate).minimize(training_cost)\n",
    "\n",
    "# For printing the intermdediate data\n",
    "correct_prediction_OP = tf.equal(tf.argmax(training_prediction,1), tf.argmax(ground_truth,1))\n",
    "accuracy_OP = tf.reduce_mean(tf.cast(correct_prediction_OP, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", training_cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", accuracy_OP)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "print(\"Starting Session...\")\n",
    "# Launch The tensorflow graph\n",
    "with tf.Session() as sess :\n",
    "    sess.run(init)\n",
    "    \n",
    "     # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # training cycle\n",
    "    for i in range(traing_epochs) :\n",
    "        step, summary =  sess.run([training_optimzer, merged_summary_op], feed_dict={x : trainX, ground_truth: trainY})\n",
    "        # Write logs at every iteration\n",
    "        summary_writer.add_summary(summary, i)\n",
    "        if i % 100 == 0 : \n",
    "            train_accuracy, newCost = sess.run(\n",
    "                [accuracy_OP, training_cost], \n",
    "                feed_dict={x: trainX, ground_truth: trainY})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, cost %g\"%(i, newCost))\n",
    "    \n",
    "    # Finally, check Test data\n",
    "    print (\"Final Accuracy on Test set: %s\" % str(sess.run(accuracy_OP, feed_dict={x: testX, ground_truth: testY})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
